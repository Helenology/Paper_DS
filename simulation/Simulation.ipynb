{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-15 15:39:39.989475: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-10-15 15:39:40.034128: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-10-15 15:39:40.738697: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.utils import array_to_img\n",
    "import tensorflow_probability as tfp\n",
    "from tensorflow.keras.layers import AveragePooling2D\n",
    "\n",
    "sys.path.append(\"../models/GPA\")\n",
    "sys.path.append(\"../models/CD\")\n",
    "sys.path.append(\"../models/DS/\")\n",
    "sys.path.append(\"../\")\n",
    "from gpa import GPA\n",
    "from cd import CD\n",
    "from ds import DS\n",
    "\n",
    "from utils import *\n",
    "from simu_auxiliary import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "⚠️ `Note`:\n",
    "\n",
    "- **TensorFlow 2.12.0** is compiled with **CUDA 11.8** and **cuDNN 8.6.0**.\n",
    "\n",
    "- If your local CUDA/cuDNN version is lower (e.g., cuDNN 8.1.x), the following line may raise a \"DNN library is not found\" or \"UnimplementedError\" due to version mismatch:\n",
    "\n",
    "> `avg_pool_2d_mean = AveragePooling2D(pool_size=(5, 5), strides=1, padding=\"same\")`\n",
    "\n",
    "- In that case, switch to Code Block (2) for an alternative CPU-based implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Code Block (1): Tensorflow GPU\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "print(gpus)\n",
    "for gpu in gpus:\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code Block (2): Tensorflow CPU\n",
    "## this code will enforce tensorflow to use cpu\n",
    "# tf.config.set_visible_devices([], 'GPU') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Hyper-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "T = 100                    # the number of replications\n",
    "N_list = [100, 500, 1000]  # the number of images\n",
    "p = 540                    # image height\n",
    "q = 960                    # image width\n",
    "M = p * q                  # the number of pixel locations\n",
    "G = 500                    # the number of grid points\n",
    "test_size = 100            # the number of test images\n",
    "filter_size = 3            # filter size for kernel smoothing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# standard deviation & mean\n",
    "sigma = 0.16218820445197027  # standard deviation\n",
    "avg_pool_2d_mean = AveragePooling2D(pool_size=(5, 5), strides=1, padding=\"same\")\n",
    "mean = np.load('./mean-540.npy').reshape([1, p, q, 1])  # mean\n",
    "mean = tf.reshape(avg_pool_2d_mean(mean), (1, p, q))    # smoothed mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# grid points: randomly generated from U[0, 1]\n",
    "np.random.seed(0)\n",
    "tf.random.set_seed(0)\n",
    "tick_list = np.random.uniform(size=G)\n",
    "grid_point = tf.concat([tf.ones([1, p, q]) * tick for tick in tick_list], axis=0)\n",
    "\n",
    "# oracle distribution\n",
    "tfd = tfp.distributions\n",
    "dist = tfd.TruncatedNormal(loc=mean, scale=sigma, low=[0.], high=[1.])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Experiments\n",
    "\n",
    "- Abbreviations:\n",
    "    - CD: Classical nonparametric Density\n",
    "    - DS: Doubly Smoothed\n",
    "    - GPA: Grid Point Approximation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "experiment_stats = {\"N\": [],            # sample size\n",
    "                    \"t\": [],            # id of the replication\n",
    "                    \"CD_mse\": [],       # MSE of the CD method\n",
    "                    \"DS_mse\": [],       # MSE of the DS method\n",
    "                    \"GPA_CD_mse\": [],   # MSE of the GPA-CD method\n",
    "                    \"GPA_DS_mse\": [],   # MSE of the GPA-DS method\n",
    "                    \"CD_time\": [],      # computation time of the CD method\n",
    "                    \"DS_time\": [],      # computation time of the DS method\n",
    "                    \"GPA_CD_time\": [],  # computation time of the GPA-CD method\n",
    "                    \"GPA_DS_time\": []}  # computation time of the GPA-DS method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# == Main experiment loop ==\n",
    "# -- Assumption: `test_imgs` and `test_orcs` are precomputed outside (above chunk),\n",
    "# -- so every method, N (sample size), and t (replication index) is evaluated\n",
    "# -- on the SAME test set for fair comparison.\n",
    "for N in N_list:\n",
    "    print(f\"====================== N={N} ======================\")\n",
    "    # -- Change path as you needed\n",
    "    path = f'../../../[simu]train_img(N={N})/'  # Directory to save N simulated images\n",
    "    bandwidth, bandwidth_star = compute_optimal_bandwidths(N, M, sigma)  # (rule-of-thumb) bandwidths\n",
    "    location_filter = get_location_filter(p, q, bandwidth, filter_size)\n",
    "    \n",
    "    # == Replicate loop (T times) ==\n",
    "    # -- Different training sets per replicate\n",
    "    for t in range(T):\n",
    "        print(f\"======== N={N}: The {t}-th replication ========\")\n",
    "        # -- Set seeds so all methods within the same replicate share training randomness\n",
    "        seed = N + t\n",
    "        tf.random.set_seed(seed)\n",
    "        np.random.seed(seed)\n",
    "        \n",
    "        # -- Initialize cumulative metrics\n",
    "        CD_mse = 0.0\n",
    "        DS_mse = 0.0\n",
    "        GPA_CD_mse = 0.0\n",
    "        GPA_DS_mse = 0.0\n",
    "        CD_time = 0.0\n",
    "        DS_time = 0.0\n",
    "        GPA_CD_time = 0.0\n",
    "        GPA_DS_time = 0.0\n",
    "        \n",
    "        # -- Generate the training set for this replicate\n",
    "        train_list = generate_simulate_data(path, N, mean, sigma)\n",
    "        \n",
    "        # -- Build models once per replicate\n",
    "        CD_model = CD(p, q, train_list)\n",
    "        \n",
    "        ### --- this will fit DS model from raw\n",
    "        # DS_model = DS(p, q, train_list) \n",
    "        \n",
    "        ### --- this will fit GPA-CD model from raw\n",
    "        # GPA_CD_model = GPA(G, p, q, train_list, second_smooth=False, gpa_matrix=None,\n",
    "        #                    grid_point=grid_point, h=bandwidth, h_star=bandwidth_star, filter_size=None)\n",
    "        \n",
    "        ### --- this will fit GPA-DS model from raw\n",
    "        # GPA_DS_model = GPA(G, p, q, train_list, second_smooth=True, gpa_matrix=None,\n",
    "        #                    grid_point=grid_point, h=bandwidth, h_star=bandwidth_star, filter_size=filter_size)\n",
    "        \n",
    "        ### --- the below computation of GPA-DS is computed based on GPA-CD, which will save time\n",
    "        ### --- this is because GPA_DS_model = GPA(xxx) will rerun the GPA-CD computation and then doubly smooth\n",
    "        ### --- thus, to save time, we directly compute GPA-CD matrix and then doubly smooth it outside the models\n",
    "        GPA_CD_matrix = compute_CD_matrix(path, N, G, p, q, bandwidth, grid_point)\n",
    "        Omega1 = tf.nn.depthwise_conv2d(tf.reshape(GPA_CD_matrix, [G, p, q, 1]), \n",
    "                                        location_filter, strides=[1, 1, 1, 1], padding='SAME')\n",
    "        Omega2 = tf.reduce_sum(location_filter)\n",
    "        GPA_DS_matrix = Omega1 / Omega2\n",
    "        GPA_CD_matrix = tf.squeeze(GPA_CD_matrix)\n",
    "        GPA_DS_matrix = tf.squeeze(GPA_DS_matrix)\n",
    "        ### --- Fit the model based on pre-computed GPA matrix\n",
    "        GPA_CD_model = GPA(G, p, q, train_list, second_smooth=False, gpa_matrix=GPA_CD_matrix,\n",
    "                           grid_point=grid_point, h=bandwidth, h_star=bandwidth_star, filter_size=None)\n",
    "        GPA_DS_model = GPA(G, p, q, train_list, second_smooth=True, gpa_matrix=GPA_DS_matrix, \n",
    "                           grid_point=grid_point, h=bandwidth, h_star=bandwidth_star, filter_size=filter_size)\n",
    "        \n",
    "        # == Evaluate four methods on the test set ==\n",
    "        for test_i in range(test_size):\n",
    "            # -- The test image and its oracle density\n",
    "            test_img = tf.ones([p, q]) * np.random.uniform()\n",
    "            test_orc = tf.squeeze(dist.prob(test_img))\n",
    "            \n",
    "            # -- CD \n",
    "            t1 = time.time()\n",
    "            CD_test = CD_model.CD_estimation(test_img, bandwidth)\n",
    "            t2 = time.time()\n",
    "            CD_time += t2 - t1\n",
    "            \n",
    "            # -- DS \n",
    "            t3 = time.time()\n",
    "            # DS_test = DS_model.DS_estimation(test_img, bandwidth, size=filter_size)\n",
    "            CD_test = tf.reshape(CD_test, (1, p, q))\n",
    "            DS_test = compute_DS_matrix(CD_test, location_filter)\n",
    "            t4 = time.time()\n",
    "            DS_time += t2 - t1 + t4 - t3\n",
    "            \n",
    "            # -- GPA-CD \n",
    "            t1 = time.time()\n",
    "            GPA_CD_test = GPA_CD_model.compute_density(test_img)\n",
    "            t2 = time.time()\n",
    "            GPA_CD_time += t2 - t1\n",
    "            \n",
    "            # -- GPA-DS \n",
    "            t1 = time.time()\n",
    "            GPA_DS_test = GPA_DS_model.compute_density(test_img)\n",
    "            t2 = time.time()\n",
    "            GPA_DS_time += t2 - t1\n",
    "\n",
    "            # -- Accumulate MSE; average over test_size after the loop\n",
    "            CD_mse     += MSE(test_orc, tf.squeeze(CD_test))\n",
    "            DS_mse     += MSE(test_orc, tf.squeeze(DS_test)) \n",
    "            GPA_CD_mse += MSE(test_orc, tf.squeeze(GPA_CD_test))\n",
    "            GPA_DS_mse += MSE(test_orc, tf.squeeze(GPA_DS_test)) \n",
    "        \n",
    "        # -- Average metrics for this replicate (divide by test_size)\n",
    "        experiment_stats[\"N\"].append(N)\n",
    "        experiment_stats[\"t\"].append(t)\n",
    "        experiment_stats[\"CD_mse\"].append(CD_mse.numpy() / test_size)\n",
    "        experiment_stats[\"DS_mse\"].append(DS_mse.numpy() / test_size)\n",
    "        experiment_stats[\"GPA_CD_mse\"].append(GPA_CD_mse.numpy() / test_size)\n",
    "        experiment_stats[\"GPA_DS_mse\"].append(GPA_DS_mse.numpy() / test_size)\n",
    "        experiment_stats[\"CD_time\"].append(CD_time / test_size)\n",
    "        experiment_stats[\"DS_time\"].append(DS_time / test_size)\n",
    "        experiment_stats[\"GPA_CD_time\"].append(GPA_CD_time / test_size)\n",
    "        experiment_stats[\"GPA_DS_time\"].append(GPA_DS_time / test_size)\n",
    "        \n",
    "        # -- Timely record the results\n",
    "        experiment_stats_csv = pd.DataFrame(experiment_stats)\n",
    "        experiment_stats_csv.to_csv(f'./results/new_simulation.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (env)",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
